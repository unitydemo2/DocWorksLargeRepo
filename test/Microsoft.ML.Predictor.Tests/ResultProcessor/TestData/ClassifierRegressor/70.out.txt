maml.exe TrainTest test=F:\data\housing.txt tr=PoissonRegression{l2=0 m=5 initwts=0.1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-13} data=F:\data\housing.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 14
   term criterion: Mean rel impr over 5 iter'ns < tol: 1.000e-7

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 5.1570e1 (**********) 
Iter 1: 2.1337e1 (...) .
Iter 2: 6.2511e0 (...) 
Iter 3: 5.7252e0 (...) 
Iter 4: 5.3167e0 (...) 
Iter 5: 4.6865e0 (...) 
Iter 6: 4.1855e0 (8.1957e-1) 
Iter 7: 3.8110e0 (1.2806e-1) 
Iter 8: 3.6970e0 (1.0973e-1) 
Iter 9: 3.6636e0 (9.0239e-2) 
Iter 10: 3.6251e0 (5.8560e-2) 
Iter 11: 3.5423e0 (3.6317e-2) 
Iter 12: 3.4372e0 (2.1753e-2) 
Iter 13: 3.4185e0 (1.6294e-2) 
Iter 14: 3.1060e0 (3.5908e-2) 
Iter 15: 3.0576e0 (3.7120e-2) 
Iter 16: 3.0237e0 (3.4303e-2) 
Iter 17: 2.9944e0 (2.9573e-2) 
Iter 18: 2.9692e0 (3.0263e-2) 
Iter 19: 2.9517e0 (1.0457e-2) .
Iter 20: 2.9450e0 (7.6462e-3) .
Iter 21: 2.9392e0 (5.7500e-3) 
Iter 22: 2.9197e0 (5.1164e-3) 
Iter 23: 2.9092e0 (4.1234e-3) ..
Iter 24: 2.9061e0 (3.1371e-3) 
Iter 25: 2.9050e0 (2.7550e-3) 
Iter 26: 2.9008e0 (2.6484e-3) 
Iter 27: 2.8987e0 (1.4508e-3) 
Iter 28: 2.8931e0 (1.1109e-3) 
Iter 29: 2.8902e0 (1.0978e-3) .
Iter 30: 2.8882e0 (1.1636e-3) 
Iter 31: 2.8849e0 (1.1040e-3) 
Iter 32: 2.8805e0 (1.2608e-3) 
Iter 33: 2.8790e0 (9.8243e-4) 
Iter 34: 2.8743e0 (1.1065e-3) 
Iter 35: 2.8648e0 (1.6275e-3) 
Iter 36: 2.8583e0 (1.8557e-3) .
Iter 37: 2.8576e0 (1.6072e-3) 
Iter 38: 2.8551e0 (1.6762e-3) 
Iter 39: 2.8541e0 (1.4143e-3) 
Iter 40: 2.8534e0 (8.0034e-4) .
Iter 41: 2.8500e0 (5.8413e-4) 
Iter 42: 2.8465e0 (7.7624e-4) 
Iter 43: 2.8428e0 (8.6431e-4) 
Iter 44: 2.8388e0 (1.0806e-3) .
Iter 45: 2.8375e0 (1.1210e-3) 
Iter 46: 2.8305e0 (1.3804e-3) 
Iter 47: 2.8279e0 (1.3177e-3) 
Iter 48: 2.8246e0 (1.2883e-3) 
Iter 49: 2.8233e0 (1.1008e-3) .
Iter 50: 2.8224e0 (1.0747e-3) 
Iter 51: 2.8217e0 (6.2152e-4) 
Iter 52: 2.8210e0 (4.8969e-4) 
Iter 53: 2.8208e0 (2.7123e-4) 
Iter 54: 2.8201e0 (2.2064e-4) 
Iter 55: 2.8187e0 (2.5895e-4) 
Iter 56: 2.8163e0 (3.8561e-4) 
Iter 57: 2.8140e0 (4.9508e-4) ..
Iter 58: 2.8139e0 (4.8539e-4) 
Iter 59: 2.8128e0 (5.2115e-4) 
Iter 60: 2.8126e0 (4.3411e-4) 
Iter 61: 2.8125e0 (2.6888e-4) 
Iter 62: 2.8122e0 (1.2588e-4) 
Iter 63: 2.8116e0 (1.6541e-4) 
Iter 64: 2.8112e0 (1.1609e-4) 
Iter 65: 2.8107e0 (1.3418e-4) 
Iter 66: 2.8105e0 (1.4343e-4) 
Iter 67: 2.8104e0 (1.3326e-4) 
Iter 68: 2.8103e0 (9.3117e-5) 
Iter 69: 2.8101e0 (7.9873e-5) 
Iter 70: 2.8098e0 (6.4165e-5) .
Iter 71: 2.8097e0 (5.2728e-5) 
Iter 72: 2.8095e0 (5.8808e-5) 
Iter 73: 2.8095e0 (5.7638e-5) 
Iter 74: 2.8095e0 (4.3467e-5) 
Iter 75: 2.8094e0 (3.0585e-5) 
Iter 76: 2.8092e0 (3.5730e-5) 
Iter 77: 2.8090e0 (3.6853e-5) ..
Iter 78: 2.8090e0 (3.4681e-5) 
Iter 79: 2.8089e0 (3.8298e-5) .
Iter 80: 2.8089e0 (3.6006e-5) 
Iter 81: 2.8088e0 (2.9878e-5) 
Iter 82: 2.8087e0 (2.2529e-5) 
Iter 83: 2.8084e0 (4.2142e-5) ..
Iter 84: 2.8084e0 (3.5656e-5) 
Iter 85: 2.8083e0 (3.8186e-5) 
Iter 86: 2.8083e0 (3.5895e-5) 
Iter 87: 2.8083e0 (2.8356e-5) 
Iter 88: 2.8083e0 (8.5917e-6) 
Iter 89: 2.8083e0 (1.1309e-5) .
Iter 90: 2.8083e0 (6.6052e-6) 
Iter 91: 2.8082e0 (6.6052e-6) 
Iter 92: 2.8082e0 (6.8090e-6) ..
Iter 93: 2.8082e0 (5.4846e-6) 
Iter 94: 2.8082e0 (3.1243e-6) .
Iter 95: 2.8082e0 (3.7187e-6) ....
Iter 96: 2.8082e0 (2.0376e-6) 
Iter 97: 2.8082e0 (1.9867e-6) 
Iter 98: 2.8082e0 (3.0734e-6) .
Iter 99: 2.8082e0 (2.9546e-6) 
Iter 100: 2.8081e0 (3.6678e-6) ..
Iter 101: 2.8081e0 (3.9225e-6) 
Iter 102: 2.8081e0 (4.2112e-6) ....
Iter 103: 2.8081e0 (3.1244e-6) 
Iter 104: 2.8081e0 (2.6829e-6) ..
Iter 105: 2.8081e0 (1.2056e-6) 
Iter 106: 2.8081e0 (1.1547e-6) .
Iter 107: 2.8081e0 (3.7357e-7) .
Iter 108: 2.8081e0 (4.4150e-7) 
Iter 109: 2.8081e0 (5.9432e-7) ..
Iter 110: 2.8081e0 (5.7734e-7) ..
Iter 111: 2.8081e0 (4.0754e-7) .....
Iter 112: 2.8081e0 (3.7357e-7) ......
Iter 113: 2.8081e0 (3.0565e-7) .......
Iter 114: 2.8081e0 (5.0942e-8)
L1 regularization selected 14 of 14 weights.
Not training a calibrator because it is not needed.
L1(avg):           2.81790170
L2(avg):           16.77167808
RMS(avg):          4.09532393
LOSS-FN(avg):      16.77167815

OVERALL RESULTS
---------------------------------------
L1(avg):             2.8179 (0.0000)
L2(avg):            16.7717 (0.0000)
RMS(avg):            4.0953 (0.0000)
LOSS-FN(avg):       16.7717 (0.0000)

---------------------------------------
2/2/2016 11:08:10 AM	 Time elapsed(s): 0.518

