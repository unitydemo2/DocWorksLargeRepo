maml.exe TrainTest test=F:\data\housing.txt tr=PoissonRegression{l2=0.1 ot=0.0001 m=5 initwts=0.5} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-13} data=F:\data\housing.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 14
   term criterion: Mean rel impr over 5 iter'ns < tol: 1.000e-4

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 6.8518e1 (**********) 
Iter 1: 3.4977e1 (...) ..
Iter 2: 1.5754e1 (...) .
Iter 3: 5.1597e0 (...) 
Iter 4: 4.8394e0 (...) 
Iter 5: 4.2952e0 (...) 
Iter 6: 4.2441e0 (1.4483e0) 
Iter 7: 4.1343e0 (5.6214e-1) 
Iter 8: 3.9706e0 (5.9899e-2) 
Iter 9: 3.8044e0 (5.4410e-2) 
Iter 10: 3.7720e0 (2.7741e-2) 
Iter 11: 3.6416e0 (3.3088e-2) 
Iter 12: 3.3873e0 (4.4108e-2) 
Iter 13: 3.1239e0 (5.4210e-2) .
Iter 14: 3.0719e0 (4.7692e-2) 
Iter 15: 3.0285e0 (4.9102e-2) 
Iter 16: 2.9986e0 (4.2892e-2) .
Iter 17: 2.9799e0 (2.7343e-2) 
Iter 18: 2.9597e0 (1.1091e-2) 
Iter 19: 2.9558e0 (7.8592e-3) 
Iter 20: 2.9523e0 (5.1570e-3) 
Iter 21: 2.9501e0 (3.2819e-3) 
Iter 22: 2.9387e0 (2.8018e-3) 
Iter 23: 2.9318e0 (1.9040e-3) 
Iter 24: 2.9280e0 (1.8975e-3) 
Iter 25: 2.9255e0 (1.8322e-3) .
Iter 26: 2.9223e0 (1.9041e-3) 
Iter 27: 2.9156e0 (1.5821e-3) 
Iter 28: 2.9048e0 (1.8595e-3) 
Iter 29: 2.8977e0 (2.0877e-3) 
Iter 30: 2.8950e0 (2.1131e-3) 
Iter 31: 2.8923e0 (2.0796e-3) 
Iter 32: 2.8897e0 (1.7926e-3) 
Iter 33: 2.8845e0 (1.4087e-3) 
Iter 34: 2.8805e0 (1.1969e-3) 
Iter 35: 2.8757e0 (1.3398e-3) 
Iter 36: 2.8733e0 (1.3167e-3) 
Iter 37: 2.8718e0 (1.2467e-3) 
Iter 38: 2.8667e0 (1.2390e-3) .
Iter 39: 2.8661e0 (1.0039e-3) 
Iter 40: 2.8627e0 (9.0752e-4) 
Iter 41: 2.8599e0 (9.4138e-4) 
Iter 42: 2.8572e0 (1.0236e-3) 
Iter 43: 2.8551e0 (8.1482e-4) 
Iter 44: 2.8542e0 (8.3228e-4) 
Iter 45: 2.8522e0 (7.3337e-4) 
Iter 46: 2.8482e0 (8.2172e-4) 
Iter 47: 2.8458e0 (8.0195e-4) 
Iter 48: 2.8445e0 (7.4660e-4) 
Iter 49: 2.8404e0 (9.7414e-4) 
Iter 50: 2.8358e0 (1.1583e-3) 
Iter 51: 2.8321e0 (1.1376e-3) ..
Iter 52: 2.8310e0 (1.0451e-3) 
Iter 53: 2.8285e0 (1.1332e-3) 
Iter 54: 2.8275e0 (9.0944e-4) 
Iter 55: 2.8269e0 (6.3082e-4) 
Iter 56: 2.8256e0 (4.6088e-4) 
Iter 57: 2.8242e0 (4.8194e-4) 
Iter 58: 2.8236e0 (3.4601e-4) 
Iter 59: 2.8224e0 (3.6748e-4) 
Iter 60: 2.8215e0 (3.7959e-4) 
Iter 61: 2.8209e0 (3.3066e-4) 
Iter 62: 2.8197e0 (3.1730e-4) 
Iter 63: 2.8191e0 (3.2052e-4) 
Iter 64: 2.8185e0 (2.7472e-4) 
Iter 65: 2.8166e0 (3.5041e-4) 
Iter 66: 2.8160e0 (3.4696e-4) 
Iter 67: 2.8157e0 (2.8369e-4) 
Iter 68: 2.8150e0 (2.8656e-4) 
Iter 69: 2.8148e0 (2.5986e-4) 
Iter 70: 2.8145e0 (1.4895e-4) 
Iter 71: 2.8143e0 (1.1942e-4) 
Iter 72: 2.8138e0 (1.3601e-4) .
Iter 73: 2.8137e0 (9.3683e-5)
L1 regularization selected 14 of 14 weights.
Not training a calibrator because it is not needed.
L1(avg):           2.74523572
L2(avg):           16.67634544
RMS(avg):          4.08366814
LOSS-FN(avg):      16.67634523

OVERALL RESULTS
---------------------------------------
L1(avg):             2.7452 (0.0000)
L2(avg):            16.6763 (0.0000)
RMS(avg):            4.0837 (0.0000)
LOSS-FN(avg):       16.6763 (0.0000)

---------------------------------------
2/2/2016 11:08:42 AM	 Time elapsed(s): 0.504

