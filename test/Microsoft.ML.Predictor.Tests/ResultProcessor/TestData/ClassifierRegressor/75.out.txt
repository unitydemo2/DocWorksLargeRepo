maml.exe TrainTest test=F:\data\housing.txt tr=PoissonRegression{l1=0.1 m=5 initwts=0.1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-13} data=F:\data\housing.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 14
   term criterion: Mean rel impr over 5 iter'ns < tol: 1.000e-7

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 5.3475e1 (**********) 
Iter 1: 3.3141e1 (...) .
Iter 2: 1.5962e1 (...) 
Iter 3: 1.2974e1 (...) 
Iter 4: 7.7662e0 (...) 
Iter 5: 6.4737e0 (...) 
Iter 6: 5.8085e0 (9.4110e-1) 
Iter 7: 5.4497e0 (3.8579e-1) 
Iter 8: 4.0105e0 (4.4700e-1) 
Iter 9: 3.7608e0 (2.1300e-1) 
Iter 10: 3.4101e0 (1.7968e-1) 
Iter 11: 3.1608e0 (1.6753e-1) ..
Iter 12: 3.1450e0 (1.4656e-1) 
Iter 13: 3.0321e0 (6.4533e-2) 
Iter 14: 2.9732e0 (5.2984e-2) 
Iter 15: 2.9575e0 (3.0605e-2) 
Iter 16: 2.9458e0 (1.4599e-2) .
Iter 17: 2.9289e0 (1.4758e-2) ..
Iter 18: 2.9242e0 (7.3850e-3) .
Iter 19: 2.9192e0 (3.6952e-3) 
Iter 20: 2.8976e0 (4.1370e-3) 
Iter 21: 2.8926e0 (3.6816e-3) 
Iter 22: 2.8874e0 (2.8742e-3) 
Iter 23: 2.8838e0 (2.7976e-3) 
Iter 24: 2.8826e0 (2.5419e-3) 
Iter 25: 2.8745e0 (1.6072e-3) 
Iter 26: 2.8686e0 (1.6723e-3) 
Iter 27: 2.8593e0 (1.9674e-3) ..
Iter 28: 2.8585e0 (1.7739e-3) 
Iter 29: 2.8559e0 (1.8705e-3) 
Iter 30: 2.8533e0 (1.4838e-3) 
Iter 31: 2.8502e0 (1.2871e-3) 
Iter 32: 2.8454e0 (9.7422e-4) .
Iter 33: 2.8433e0 (1.0696e-3) .
Iter 34: 2.8383e0 (1.2356e-3) 
Iter 35: 2.8348e0 (1.3062e-3) 
Iter 36: 2.8342e0 (1.1332e-3) 
Iter 37: 2.8293e0 (1.1432e-3) 
Iter 38: 2.8254e0 (1.2652e-3) 
Iter 39: 2.8229e0 (1.0956e-3) 
Iter 40: 2.8217e0 (9.2473e-4) .
Iter 41: 2.8211e0 (9.2527e-4) 
Iter 42: 2.8195e0 (6.9104e-4) 
Iter 43: 2.8181e0 (5.1521e-4) 
Iter 44: 2.8143e0 (6.0938e-4) 
Iter 45: 2.8127e0 (6.4483e-4) .
Iter 46: 2.8120e0 (6.5070e-4) .
Iter 47: 2.8116e0 (5.6253e-4) 
Iter 48: 2.8110e0 (5.0977e-4) .
Iter 49: 2.8104e0 (2.8123e-4) .
Iter 50: 2.8099e0 (1.9573e-4) 
Iter 51: 2.8088e0 (2.2512e-4) 
Iter 52: 2.8082e0 (2.4504e-4) .
Iter 53: 2.8075e0 (2.4343e-4) 
Iter 54: 2.8070e0 (2.4151e-4) 
Iter 55: 2.8058e0 (2.9083e-4) ..
Iter 56: 2.8057e0 (2.2456e-4) 
Iter 57: 2.8052e0 (2.1558e-4) 
Iter 58: 2.8043e0 (2.2809e-4) 
Iter 59: 2.8040e0 (2.1072e-4) 
Iter 60: 2.8038e0 (1.4401e-4) 
Iter 61: 2.8035e0 (1.5381e-4) 
Iter 62: 2.8035e0 (1.1836e-4) 
Iter 63: 2.8032e0 (8.1906e-5) 
Iter 64: 2.8031e0 (6.3043e-5) 
Iter 65: 2.8030e0 (5.9711e-5) .
Iter 66: 2.8029e0 (4.7380e-5) 
Iter 67: 2.8026e0 (6.0944e-5) 
Iter 68: 2.8026e0 (3.9710e-5) 
Iter 69: 2.8025e0 (4.6689e-5) 
Iter 70: 2.8025e0 (3.8437e-5) 
Iter 71: 2.8024e0 (2.9215e-5) 
Iter 72: 2.8024e0 (1.5467e-5) 
Iter 73: 2.8024e0 (1.6335e-5) 
Iter 74: 2.8023e0 (1.2354e-5) ....
Iter 75: 2.8023e0 (1.1043e-5) 
Iter 76: 2.8023e0 (1.3613e-5) 
Iter 77: 2.8022e0 (1.7187e-5) .
Iter 78: 2.8022e0 (1.7170e-5) 
Iter 79: 2.8021e0 (1.1435e-5) 
Iter 80: 2.8021e0 (1.2116e-5) 
Iter 81: 2.8021e0 (9.7167e-6) 
Iter 82: 2.8021e0 (7.3174e-6) .
Iter 83: 2.8021e0 (7.8961e-6) 
Iter 84: 2.8020e0 (8.6960e-6) 
Iter 85: 2.8020e0 (9.5640e-6) 
Iter 86: 2.8020e0 (9.1386e-6) 
Iter 87: 2.8020e0 (8.0495e-6) 
Iter 88: 2.8020e0 (6.8582e-6) 
Iter 89: 2.8019e0 (6.0925e-6) .
Iter 90: 2.8019e0 (5.0885e-6) 
Iter 91: 2.8019e0 (5.6841e-6) 
Iter 92: 2.8019e0 (5.6671e-6) 
Iter 93: 2.8019e0 (5.1906e-6) ..
Iter 94: 2.8019e0 (4.5780e-6) 
Iter 95: 2.8018e0 (6.1268e-6) 
Iter 96: 2.8018e0 (5.4800e-6) .
Iter 97: 2.8018e0 (4.4249e-6) 
Iter 98: 2.8018e0 (5.5652e-6) .
Iter 99: 2.8018e0 (4.3568e-6) ..
Iter 100: 2.8018e0 (2.5358e-6) ....... ........
Iter 101: 2.8018e0 (2.2805e-6) .......
Iter 102: 2.8018e0 (2.1784e-6) .......
Iter 103: 2.8018e0 (8.1691e-7) .......
Iter 104: 2.8018e0 (2.7230e-7) .......
Iter 105: 2.8018e0 (0.0000e0)
L1 regularization selected 14 of 14 weights.
Not training a calibrator because it is not needed.
L1(avg):           2.81581041
L2(avg):           16.79263051
RMS(avg):          4.09788122
LOSS-FN(avg):      16.79263040

OVERALL RESULTS
---------------------------------------
L1(avg):             2.8158 (0.0000)
L2(avg):            16.7926 (0.0000)
RMS(avg):            4.0979 (0.0000)
LOSS-FN(avg):       16.7926 (0.0000)

---------------------------------------
2/2/2016 11:08:40 AM	 Time elapsed(s): 0.506

